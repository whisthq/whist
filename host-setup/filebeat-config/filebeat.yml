############################# Input #####################################
filebeat.inputs:
  - type: log
    paths:
      - /fractal/temp/logs/*/*/*.log

    fields:
      # For JSON logs, set to `json`. For plain log lines, set to `plain`
      logzio_codec: plain

      # Your Logz.io account token. You can find your token at
      #  https://app.logz.io/#/dashboard/settings/manage-accounts
      token: DUMMY_TOKEN_REPLACE_ME

      # Logz.io uses log type to parse your logs. Logs of the same type are
      #  parsed the same way. For more information, see
      #  https://docs.logz.io/user-guide/log-shipping/built-in-log-types.html
      #
      # For custom parsing you can:
      #  - Configure Logstash in your system
      #  - Use the Data Parsing wizard
      #    (https://app.logz.io/#/dashboard/data-parsing/)
      #  - Get help from the Support team (email help@logz.io)
      # type: LOG-TYPE

    # fields_under_root must be set to `true`. Do not change this option.
    fields_under_root: true
    encoding: utf-8
    ignore_older: 3h

    # If your plain text logs span multiple lines, uncomment the `multiline`
    # option. On the `pattern` line, give a Filebeat-supported regex pattern.
    # Filebeat combines lines when it finds the regex pattern. Supported regex
    # patterns are at https://www.elastic.co/guide/en/beats/filebeat/current/regexp-support.html
    #multiline:
    #  pattern: # Regex pattern to match
    #  negate: true
    #  match: after

# For Filebeat 7.x, use this line and remove the `registry_file` line above
filebeat.registry.path: /var/lib/filebeat

# Processors can be used to filter and enhance data that's being captured from log files.
# https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html
processors:
  - add_fields:
      fields:
        environment: local-dev
  - rename:
      fields:
        - from: "agent"
          to: "filebeat_agent"
      ignore_missing: true
  - dissect:
      # https://www.elastic.co/guide/en/beats/filebeat/current/dissect.html
      # We use the dissect processor to parse out the metadata for logs based on file-path.
      # Logs are mounted from mandelboxes onto host through bind mounts decalared in `ecs-host-service/ecs-host-service.go`
      # A filepath looks like below.
      # /fractal/temp/logs/6137d7df55ecba06846ab1b5670121335f31a30f045ecfe6f31acdf09a3e/protocol-out.log
      tokenizer: "/fractal/temp/logs/%{container_id}/%{session_id}/%{component}-%{log_level}.log"
      field: "log.file.path"
      target_prefix: ""
      # setting ignore_failure to false will stop execection of subsequent processors if dissect errors out for some reason.
      ignore_failure: true

  # https://www.elastic.co/guide/en/beats/filebeat/current/add-cloud-metadata.html
  - add_cloud_metadata:
      # timeout: 3s
      providers:
        - aws

############################# Output ##########################################
output:
  logstash:
    # Use the Logz.io listener URL for your region. For help finding your
    # region's listener URL, see:
    # https://docs.logz.io/user-guide/accounts/account-region.html
    hosts: ["listener.logz.io:5015"]

    ssl:
      # For Linux, download the Logz.io public certificate to this folder.
      # Run this command:
      #   `sudo curl https://raw.githubusercontent.com/logzio/public-certificates/master/TrustExternalCARoot_and_USERTrustRSAAAACA.crt --create-dirs -o /etc/pki/tls/certs/COMODORSADomainValidationSecureServerCA.crt`
      # For Windows, replace the filepath with the certificate's location.
      certificate_authorities:
        - "/etc/pki/tls/certs/COMODORSADomainValidationSecureServerCA.crt"
